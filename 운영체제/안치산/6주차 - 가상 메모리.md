# ChatGPT 심층 리서치 결과

가상 메모리: 원리와 주요 개념

가상 메모리의 개념과 목적

가상 메모리를 활용하면 각 프로세스는 연속적이고 독자적인 큰 주소 공간(노란색)을 가지는 것처럼 동작하지만, 실제 물리 메모리(초록색 부분)에서는 여러 프로세스의 페이지들이 뒤섞여 배치되고 일부 페이지는 디스크 스왑 영역(빨간색)으로 옮겨질 수 있다. 운영체제는 주기억장치(RAM)와 보조기억장치(디스크)를 조합하여 프로세스마다 거대하고 연속적인 가상 주소 공간을 제공한다 ￼. 이를 통해 프로그램은 자신의 전체 메모리 공간을 마치 연속된 배열처럼 취급할 수 있으며, 실제 물리 메모리 크기에 구애받지 않고 필요한 만큼 메모리를 사용하는 “환상”을 얻는다 ￼ ￼. 예를 들어 물리 메모리가 8GB인 시스템에서도 각 프로세스는 8GB를 훨씬 상회하는 메모리를 할당받는 것처럼 동작할 수 있다. 또한 가상 메모리는 프로세스 간 메모리 충돌을 방지하여 메모리 보호를 구현하고, 여러 프로세스가 라이브러리 코드 등을 동시에 사용할 경우 해당 메모리 영역을 공유함으로써 메모리 사용 효율을 높이며, 프로세스별 메모리 공간을 격리해 보안을 강화하는 등의 이점도 제공한다 ￼. 요약하면, 가상 메모리는 프로그램을 물리 메모리 제약에서 해방하고 메모리 관리의 복잡성을 운영체제가 맡도록 함으로써 시스템 안정성과 확장성을 향상시키는 기술이다 ￼.

가상 메모리의 주요 목적 중 하나는 실제 메모리보다 큰 주소 공간을 프로그램에 제공하는 것이다 ￼. 프로그램 전체를 한꺼번에 메모리에 올리지 않고 실제로 필요한 부분만 메모리에 적재함으로써, 한정된 메인 메모리를 보다 효율적으로 활용할 수 있다 ￼. 그 결과 더 많은 프로그램을 동시에 실행(다중 프로그래밍 수준 향상)할 수 있고, 각 프로세스는 연속적이고 고립된 메모리 공간을 갖게 되어 프로그래밍이 단순해진다 ￼. 실제로 가상 메모리 도입 전에는 개발자가 물리 메모리 크기에 맞춰 프로그램이 사용할 메모리를 수동 관리하거나 오버레이 기법 등을 사용해야 했지만, 가상 메모리 시스템에서는 이러한 부담이 사라진다. 반면 가상 메모리를 실현하기 위해 주소 변환과 디스크 스와핑 등의 부가 작업이 필요해지므로, 운영체제는 하드웨어의 지원을 받아 성능 손실을 최소화하도록 설계되어야 한다 ￼.

주소 공간과 논리/물리/가상 주소

각 프로세스는 독자적인 **주소 공간(address space)**을 가지며, 이 주소 공간 내에서 논리 주소(logical address) 혹은 가상 주소(virtual address)를 사용하여 메모리에 접근한다 ￼. 즉, 실행 중인 프로그램이 다루는 메모리 참조는 실제 RAM의 물리적 위치가 아닌, 운영체제가 제공한 가상의 주소로 표현된다. 이러한 가상 주소는 CPU가 명령을 수행하면서 생성하는 주소이고, 물리 주소(physical address)는 메인 메모리에서의 실제 위치를 가리킨다 ￼. 운영체제와 하드웨어는 **주소 변환(address translation)**을 통해 논리/가상 주소를 대응되는 물리 주소로 변환하며, 이 과정은 CPU 내 **메모리 관리 유닛(MMU)**에 의해 실행 시간에 자동으로 이루어진다 ￼ ￼.

주소 변환 과정에서 **페이지 테이블(page table)**이라는 매핑 테이블이 핵심적으로 사용되며, 각 프로세스마다 자신만의 페이지 테이블을 유지한다. 결과적으로 서로 다른 프로세스는 동일한 논리 주소값을 사용하더라도 각기 다른 물리 메모리 위치로 매핑되므로 프로세스 간 메모리 격리가 가능해진다. 예를 들어 프로세스 A와 B가 모두 논리 주소 0x1000을 읽더라도, A의 0x1000과 B의 0x1000은 서로 다른 물리 메모리 프레임에 매핑될 수 있다. 또한 한 프로세스의 가상 주소 공간 변경(메모리 할당이나 해제 등)은 다른 프로세스의 주소 공간에는 전혀 영향을 주지 않는다. 이처럼 논리/가상 주소와 물리 주소의 분리를 통해 운영체제는 각 프로그램에 독립된 메모리 환경을 제공하고, 메모리 보호와 효율적 분배를 구현한다 ￼ ￼.

주소 공간의 구성과 크기

32비트 시스템에서는 일반적으로 각 프로세스에 최대 4GB의 가상 주소 공간이 주어지는데, 이 중 일부 상위 영역은 운영체제 커널이 사용하고 나머지를 사용자 프로세스가 사용한다. 예를 들어 32비트 리눅스의 경우 프로세스별 가상 주소 공간을 상위 1GB는 커널, 하위 3GB는 사용자 영역으로 나누는데 ￼, 이를 통해 모든 프로세스가 동일한 커널 공간을 공유하면서도 서로의 사용자 공간에는 접근하지 못하도록 한다. 64비트 환경에서는 가상 주소 공간이 매우 방대하여(이론상 수십 엑사바이트 이상) 이러한 고정된 3GB/1GB 분할이 없지만, 여전히 일부 상위 주소 범위를 커널 전용으로 예약하고 나머지를 사용자 공간으로 할당하는 방식으로 주소 공간을 관리한다. 예를 들어 x86-64 리눅스에서는 상위 절반 가량의 주소(예: 0xFFFF_8000_0000_0000 이상)를 커널이 사용하고 나머지 하위 영역을 각 프로세스에 할당한다. 이로써 커널 공간과 유저 공간의 구분이 이루어져, 사용자 프로세스는 허가 없이 커널 영역을 접근할 수 없고 커널은 모든 프로세스 메모리에 접근할 수 있는 구조가 된다 ￼ ￼.

페이지 테이블의 구조 및 역할

페이지 테이블은 가상 메모리 시스템의 핵심 자료구조로, 가상 주소를 물리 주소로 매핑하는 정보를 담고 있다 ￼. 각 프로세스는 자신의 페이지 테이블을 가지고 있으며, 이 테이블의 엔트리 하나하나(PTE, Page Table Entry)가 “가상 페이지 번호 -> 물리 프레임 번호”의 대응 관계를 나타낸다 ￼. CPU가 어떤 가상 주소에 접근하면, 그 주소의 상위 비트로부터 가상 페이지 번호(page number)를 얻어 해당 프로세스의 페이지 테이블을 조회하고, 거기 기록된 프레임 번호(frame number)를 통해 실제 물리 메모리의 위치를 알아낸다 ￼ ￼. 이렇게 찾아낸 물리 주소를 사용하여 메모리에 접근함으로써, 프로그램의 가상 주소 참조가 실제 하드웨어 메모리로 연결된다.

페이지 테이블의 구성 요소로는 매핑 정보인 프레임 번호 외에도, 해당 페이지의 상태를 나타내는 여러 **제어 비트(control bits)**가 포함된다 ￼. 예를 들어 유효(Valid)/무효(Invalid) 비트는 해당 가상 페이지가 현재 메모리에 올라와 있는지(유효) 아니면 디스크 등에 있으며 아직 메모리에 없는지(무효)를 표시한다 ￼. 이 비트가 1이면 페이지가 메모리에 있음을 뜻하고, 0이면 현재 메모리에 없다는 뜻으로 페이지 부재(page fault) 시 처리 대상이 된다 ￼. 그 밖에도 **보호 비트(protection bits)**는 해당 페이지가 읽기/쓰기/실행 중 어떤 접근을 허용하는지를 지정하며, **참조 비트(reference bit)**는 최근에 접근되었는지 여부를 표시하고, **변경 비트(dirty bit)**는 페이지가 메모리 상에서 수정되었지만 아직 디스크에 반영되지 않았음을 나타낸다. 이러한 메타데이터를 통해 운영체제는 메모리 보호를 구현하고 페이지 교체 등의 전략을 결정한다.

현대 컴퓨터의 주소 공간은 매우 크기 때문에 페이지 테이블 또한 거대해질 수 있으며, 모든 엔트리를 일일이 물리 메모리에 유지하면 부담이 된다. 이를 해결하기 위해 계층적 페이지 테이블(multilevel page table) 구조를 사용한다. 가령 64비트 x86 리눅스에서는 가상 주소를 최대 5단계로 나누어 한 번에 전체를 거대한 테이블로 매핑하지 않고 여러 레벨의 테이블을 거쳐가는 방식으로 주소 변환을 수행한다. 이렇게 하면 실제로 사용하는 가상 주소 영역에 해당하는 테이블 부분만 메모리에 두면 되므로 페이지 테이블로 인한 메모리 낭비를 줄일 수 있다. 그 외에 프로세스 ID와 페이지 번호를 키로 삼는 역페이지 테이블(Inverted Page Table) 기법도 존재하는데 ￼, 이는 시스템 전역에 하나의 해시 테이블을 두어 페이지를 찾는 방법으로, 주소 공간이 매우 큰 시스템에서 테이블 크기를 줄이는 데 쓰인다 (일반 PC보다는 대형 서버나 특수 OS 등에서 사용). 하지만 전통적인 계층적 페이지 테이블이 주류이며, 하드웨어의 지원 하에 다단계 페이지 테이블을 빠르게 탐색하는 방식이 널리 쓰인다.

TLB(Translation Lookaside Buffer)의 동작 원리

페이지 테이블을 통한 주소 변환은 필수적이지만, 모든 메모리 접근마다 페이지 테이블을 조회한다면 메모리 접근 횟수가 배로 늘어나는 셈이어서 시스템 성능이 크게 저하될 수 있다 ￼. 이를 보완하기 위해 **TLB(변환 색인 버퍼)**라는 하드웨어 캐시가 사용된다. TLB는 CPU MMU의 일부로 내장된 주소 변환 캐시이며, 최근에 참조된 가상->물리 주소 매핑 정보(PTE 일부)를 저장해 둔다 ￼. 메모리 참조가 발생하면 먼저 TLB에서 해당 가상 페이지의 매핑을 찾고, 있다면 곧바로 물리 주소로 변환하여 메모리에 접근한다 ￼. 이때는 페이지 테이블에 접근할 필요가 없으므로 한 번의 메모리 접근만으로 데이터에 도달하게 되어 매우 빠르다. 만약 TLB에 해당 항목이 없다(TLB 미스)면 그때 페이지 테이블을 읽어 매핑 정보를 가져오고, 동시에 그 정보를 TLB에 적재하여 이후에 재사용할 수 있게 한다. 이러한 TLB 히트/미스 비율은 주소 변환 성능에 결정적인 영향을 미치는데, 프로그램의 지역성(locality) 특성 덕분에 일반적으로 TLB 적중률은 높게(90~99% 이상) 유지된다.

가상 주소를 물리 주소로 변환하는 과정의 예시: CPU는 먼저 TLB에서 매핑을 찾고(TLB hit일 경우 즉시 물리 주소로 변환) 없으면 TLB 미스가 발생하여 페이지 테이블을 조회한다. 페이지 테이블에 해당 페이지의 매핑이 있으면(page table hit) 해당 프레임을 얻어와 TLB에 기록한 뒤 변환을 완료하지만, 페이지 테이블에도 없으면(page not present) 운영체제가 디스크로부터 해당 페이지를 읽어와 메모리에 적재하고 페이지 테이블 및 TLB를 갱신한다. TLB 미스로 인한 페이지 테이블 접근이나 페이지 부재 발생 시 디스크 I/O가 추가로 일어나기 때문에 주소 변환에 큰 지연이 생긴다. 이처럼 TLB는 자주 사용되는 주소 변환 결과를 캐시함으로써 페이지 테이블 조회 빈도를 크게 줄여준다 ￼. 하드웨어적으로 TLB는 소수의 엔트리를 가지는 고속 연관 캐시(associative cache) 구조로 구현되어, 병렬 비교로 해당 가상 페이지의 매핑을 매우 빠르게 찾아낸다. TLB 적중 시에는 주소 변환이 한 사이클 내에 완료되지만, 실패 시에는 페이지 테이블 메모리 접근(또는 디스크 접근)까지 거치므로 수십에서 수백 사이클 이상의 지연이 발생할 수 있다 ￼.

TLB의 효율적인 활용을 위해 운영체제는 다양한 정책을 쓴다. 일반적으로 **문맥 교환(context switch)**으로 프로세스가 변경되면, 새로운 프로세스의 주소 공간에 대한 TLB 엔트리가 기존에는 없으므로 TLB 미스율이 일시적으로 증가한다. 특히 한 프로세스에서 다른 프로세스로 전환할 때 TLB에 남아 있던 이전 프로세스의 매핑은 무효화되어야 하므로, 과거에는 프로세스 교체 시마다 TLB 전체를 플러시(flush) 하는 방법을 썼다. 이렇게 하면 안전하지만 TLB 내용을 모두 잃게 되어 성능에 불리하다. 현대 CPU는 이를 개선하고자 주소 공간 식별자(ASID) 또는 PCID(Process Context ID)와 같은 태그를 도입하여, TLB 엔트리를 프로세스 구분까지 포함해 저장함으로써 굳이 TLB를 비우지 않고도 프로세스 전환이 가능하게 하였다 ￼. 예컨대 x86 CPU의 CR3 레지스터(페이지 테이블 기준 레지스터)를 새 값으로 로드하면 기본적으로 TLB가 플러시되지만 ￼, PCID 기능을 사용하면 서로 다른 PCID에 해당하는 TLB 엔트리끼리는 간섭하지 않도록 해 프로세스 전환 시에도 일부 TLB 엔트리를 유지할 수 있다 ￼. 다만 PCID/ASID의 개수 제한(예: 4096개) 등으로 완전한 해결책은 아니며, 현재의 OS들은 보안 이슈(멜트다운 대응 등)로 인해 의도적으로 컨텍스트 전환 시 TLB를 flush하거나 PCID를 리셋하는 경우도 있다. 그럼에도 불구하고 일반적인 작업에서는 TLB flush를 최소화하고 가능한 한 높은 TLB 적중률을 유지하는 것이 성능에 매우 중요하다.

마지막으로, TLB 엔트리가 부족한 경우 오래된 항목을 교체하는 TLB 교체 알고리즘도 하드웨어 내에 존재한다. 대부분의 경우 간단한 LRU 근사 알고리즘으로 동작하며, 자세한 제어는 하드웨어(MMU)가 자동으로 수행하므로 소프트웨어 개발자는 직접 TLB 교체를 다룰 필요는 없다. 다만 TLB와 캐시의 관계, TLB 미스가 시스템 퍼포먼스에 미치는 영향 등을 이해하고 있으면 메모리 지역성 향상이나 페이지 크기 조정(예: huge page 사용) 등의 튜닝에 응용할 수 있다.

페이지 교체 알고리즘 (FIFO, LRU, LFU 등)

운영체제가 각 프로세스별로 가상 메모리를 제공할 때, 실제 물리 메모리의 크기는 한정되어 있기 때문에 모든 프로세스의 모든 페이지를 한꺼번에 메모리에 올려둘 수는 없다. 따라서 일부 페이지는 디스크의 스왑 영역에 보관해두다가 필요할 때 다시 메모리에 불러오는 방식(페이징 기법)을 사용하게 된다. 이 과정에서 어느 페이지를 메모리에서 내보낼지 결정하는 전략이 **페이지 교체 알고리즘(page replacement algorithm)**이다. 메모리가 가득 찬 상태에서 새로운 페이지를 적재해야 할 경우, 운영체제는 현재 메모리에 있는 페이지들 중 하나를 선택하여 내쫓고(디스크로 내려쓰고) 그 자리를 비우게 된다. **희생 페이지(victim)**로 어떤 페이지를 선택하느냐에 따라 이후 성능이 크게 좌우되므로, 페이지 부재 발생 횟수를 최소화하면서도 알고리즘 자체의 구현과 실행 오버헤드가 과도하지 않은 방향으로 설계하는 것이 중요하다 ￼.

대표적인 페이지 교체 알고리즘으로는 FIFO, Optimal(OPT), LRU, LFU 등이 있다 ￼ ￼. 각각의 개념은 다음과 같다:
	•	FIFO (First-In First-Out, 선입선출) – 가장 먼저 메모리에 올라온 페이지, 즉 가장 오랫동안 메모리에 있었던 페이지를 교체하는 방식이다 ￼. 운영체제는 메모리에 있는 페이지들을 큐(queue)로 관리하여, 그 **헤드(head)**에 위치한 오래된 페이지를 희생시킨다 ￼. 새로 적재된 페이지는 큐의 **꼬리(tail)**에 추가되며, 이렇게 하면 구현이 매우 단순하고 관리가 쉬운 장점이 있다 ￼. 그러나 이 알고리즘은 페이지가 오래 있었다는 사실만 고려할 뿐 실제 사용 빈도나 최신 사용 시점은 고려하지 않으므로 비효율적인 교체가 발생할 수 있다 ￼. 극단적인 경우 **Belady의 모순(Belady’s Anomaly)**이라 불리는 현상이 나타나기도 하는데, 이는 FIFO에서는 프레임 수를 늘려도 오히려 페이지 폴트가 증가할 수 있다는 놀라운 결과이다 ￼. 예컨대 특정 메모리 접근 패턴에서 프레임 3개로 실행할 때보다 4개로 실행할 때 페이지 폴트 횟수가 더 많아질 수 있으며 ￼, 이러한 FIFO의 단점 때문에 실무에서는 거의 단독으로 사용되지 않는다.
	•	OPT (Optimal, 최적 알고리즘) – 이론적으로 가장 낮은 페이지 부재율을 보장하는 알고리즘으로, 앞으로 가장 오랫동안 사용되지 않을 페이지를 교체하는 방식이다 ￼. 미래의 메모리 접근 패턴을 모두 안다고 가정하고 결정하는 최적해이므로 현실에서는 구현이 불가능하지만, 다른 알고리즘들의 성능을 평가하는 **기준선(baseline)**으로 사용된다 ￼. 알고리즘 연구나 교육용 시뮬레이션 등에서 참고되며, OPT와의 격차를 줄이는 것이 다른 알고리즘들의 목표가 된다.
	•	LRU (Least Recently Used, 최근 가장 사용되지 않음) – 가장 오랫동안 사용되지 않은 페이지를 교체하는 알고리즘이다 ￼. 이는 **시간적 지역성(Temporal Locality)**에 근거한 것으로, “가장 오래 사용되지 않은 페이지는 앞으로도 당분간 사용되지 않을 것이다”라는 가정을 한다 ￼. 과거의 접근 이력을 바탕으로 미래를 예측하려는 방법으로, 현실적으로 OPT에 가까운 낮은 페이지 부재율을 달성하여 성능이 우수한 편이다 ￼ ￼. LRU 구현을 위해서는 각 페이지의 최근 사용 시점을 기록하거나 카운터/타임스탬프를 두고 관리해야 하므로 구현 복잡도와 오버헤드가 높다 ￼. 예를 들어 매 메모리 참조마다 해당 페이지의 최근 사용 시간을 갱신해야 하며, 페이지 교체 시에는 전체 페이지 중 가장 오랫동안 안 쓰인 것을 찾아야 한다. 이러한 부담 때문에 하드웨어적 지원(예: 참조 비트 제공) 없이 순수 소프트웨어만으로 LRU를 완벽히 구현하기는 어렵다 ￼. 대신에 운영체제는 LRU에 **근사한 알고리즘(approximation)**들을 사용한다. 그 중 하나가 Second-Chance (2차 기회) 알고리즘인데, 이는 FIFO 방식으로 큐의 가장 오래된 페이지를 후보로 삼되 그 페이지의 참조 비트를 확인하여 최근에 한 번이라도 사용된 페이지면 즉시 내보내지 않고 참조 비트를 0으로 리셋한 후 큐의 뒤로 보내 “한 번 더 기회”를 주는 방식이다 ￼. Second-Chance 알고리즘을 원형으로 구현한 것이 Clock 알고리즘으로, 시계 바늘이 페이지들을 돌면서 참조 비트를 검사해 0인 페이지를 교체하고 1인 페이지는 0으로 만들면서 넘어가는 절차를 가진다 ￼. 이러한 방식들은 LRU에 근접한 효율을 내면서도 구현은 단순하고 오버헤드는 줄인다는 장점이 있다.
	•	LFU (Least Frequently Used, 최저 빈도 사용) – 참조 횟수가 가장 적은 페이지를 교체하는 알고리즘이다. 페이지마다 참조된 횟수를 세어두고 가장 적게 참조된 페이지를 희생시키는 것으로, 사용 빈도에 초점을 맞춘 방법이다 ￼. 구현은 각 페이지에 카운터를 두고 메모리 참조 시 증가시키면 되므로 직관적이지만, 단점은 오래된 히스토리의 영향을 계속 누적한다는 점이다. 예를 들어 과거에는 자주 쓰였지만 최근엔 쓰이지 않는 페이지가 LFU에서는 높은 카운터 값 때문에 계속 남아있게 되고, 반대로 최근 짧은 기간에 집중적으로 쓰인 페이지가 있어도 누적 횟수가 적다면 교체 대상이 될 수 있다. 이런 이유로 LFU 단독으로 쓰이는 일은 드물고, LRU와 조합하거나 가중치 감소 기법(decay) 등을 통해 최근 사용에 더 비중을 두는 방식으로 응용된다. (일부 시스템에서는 LFU의 반대 개념인 MFU도 언급되는데, 이는 “사용 횟수가 가장 많은 페이지가 다른 페이지들도 많을 것”이라는 가정으로 오히려 가장 많이 사용된 페이지를 교체하는 실험적 알고리즘이다. 하지만 실제로 MFU가 유리한 경우는 거의 없어 이론적 언급에 그친다.)

정리하면, 페이지 교체 알고리즘은 페이지 부재율을 최소화하여 디스크 I/O를 줄이고 CPU가 유휴 상태로 보내는 시간을 감소시키는 것이 목표다 ￼. 동시에 알고리즘 자체의 메모리/CPU 오버헤드도 고려해야 한다. 운영체제는 종합적인 판단으로 알고리즘을 선택하거나 혼합 적용하며, 하드웨어 제공 정보(참조 비트, 수정 비트 등)를 최대한 활용해 효율을 높인다 ￼. 예를 들어 Linux를 비롯한 현대 OS의 페이지 교체는 LRU를 기반으로 하되, 실제 구현은 LRU를 완벽히 추적하지 않고 Clock 알고리즘의 변형을 사용하거나, 활성/비활성 페이지 리스트를 운용하면서 일정 기준으로 LRU 비슷하게 동작하도록 한다 ￼. Linux 커널의 파일 캐시 등에는 Clock-Pro라는 고급 알고리즘이 도입되어, LRU에 사용 빈도 정보를 가미하여 보다 나은 교체 판단을 수행하기도 한다 ￼. 중요한 것은 어느 알고리즘을 쓰더라도 메모리 접근의 지역성을 잘 활용하는 것이고, 이를 위해 현대 OS들은 워킹셋(Working Set) 모델을 참고하여 각 프로세스가 일정 시간 창(window) 내에 필요로 하는 페이지 집합을 예측하고 메모리에 유지하려고 노력한다. 만약 프로세스들의 총 워킹셋 크기가 물리 메모리보다 커지면 **스래싱(thrashing)**이 발생하여 페이지 부재 처리에 CPU 시간을 다 소비하게 되므로 ￼, 운영체제는 필요시 프로세스 수 조정(Contexts 줄이기) 또는 프로세스 일시 중지/스왑 아웃 등을 통해 균형을 맞추게 된다.

Demand Paging의 원리 (요구 페이징)

가상 메모리 시스템에서는 **Demand Paging(요구 페이징)**을 통해 메모리를 효율적으로 관리한다. 요구 페이징이란, 프로그램의 페이지들을 필요할 때까지 로드하지 않고 미뤄두는 기법을 말한다 ￼. 프로그램을 시작할 때 그 전체를 메모리에 미리 올리지 않고, 실행 도중에 해당 페이지에 실제로 접근이 일어날 때 디스크에서 메모리로 가져오는 방식이다 ￼. 이는 초기 로드 시간도 줄이고 메모리도 절약하는 효과가 있어서, 대부분의 현대 운영체제가 채택하고 있는 전략이다. 예를 들어 어떤 대형 실행 파일을 실행할 때, 코드와 데이터 페이지 수천 개 중 실제로 당장 필요한 페이지(예: 실행 시작점 근처 코드, 초기화된 데이터 등)만 메모리에 적재하고 나머지 코드/데이터는 디스크 상에 둔 채로 시작한다. 이후 해당 부분을 호출하거나 접근하려 할 때 페이지 폴트가 발생하고, 그 시점에 운영체제가 해당 페이지를 읽어 들여 메모리에 적재한다.

이처럼 **페이지 폴트(page fault)**는 프로그램이 **현재 메모리에 없던 페이지에 접근할 때 발생하는 예외(trap)**이다 ￼. 페이지 폴트가 일어나면 CPU는 하드웨어적으로 MMU를 통해 트랩을 발생시켜 운영체제 커널의 페이지 폴트 핸들러를 호출한다. 페이지 폴트 처리 과정은 다음과 같다 ￼ ￼:
	1.	유효성 검사 – 접근한 가상 주소가 현재 유효(valid)한 주소인지 확인한다. 만약 잘못된 주소(아예 할당되지 않은 메모리 등)라면 페이지 폴트를 복구할 수 없으므로 해당 프로세스에 세그멘테이션 오류(segfault) 신호를 보내 프로그램을 종료시키거나 잘못을 처리한다 ￼. 반면, 주소 자체는 유효하지만 단지 그 페이지가 아직 메모리에 없는 경우라면(타당 비트 0인 경우) 복구 절차를 진행한다.
	2.	빈 프레임 확보 – 우선 메모리에 빈 페이지 프레임이 있는지 찾는다 ￼. 빈 프레임이 있으면 그 프레임을 사용할 수 있고, 없으면 페이지 교체 알고리즘을 통해 기존 프레임 중 하나를 선정하여 비워야 한다 ￼. 만약 선택된 페이지가 수정된(dirty) 상태라면 이를 디스크에 **스왑 아웃(swap out)**하는 작업도 필요하다 ￼. 빈 프레임이 준비되면 그 프레임에 이제 새로운 데이터를 채울 수 있다.
	3.	디스크에서 페이지 로드 – 페이지 폴트를 일으킨 해당 가상 페이지의 내용을 디스크(스왑 공간 또는 메모리 매핑된 파일 등)에서 읽어와 확보된 빈 프레임에 저장한다 ￼. 이 단계에서는 디스크 I/O가 발생하므로 지연이 가장 크다. 운영체제는 디스크 읽기 요청을 보내놓고, 해당 프로세스를 블록(block)시킨 뒤 다른 프로세스를 수행시켜 CPU를 유휴 상태로 두지 않도록 한다. 디스크에서 페이지를 모두 읽으면 OS는 해당 프레임에 데이터가 채워졌음을 확인한다.
	4.	페이지 테이블 및 TLB 갱신 – 메모리에 새로 적재된 페이지에 대한 매핑 정보를 페이지 테이블에 업데이트하고, 유효 비트를 1로 설정한다 ￼ ￼. 또한 해당 항목이 TLB에 있었다면(이전에 사용된 적 있는 페이지였다면) 일관성을 위해 TLB 엔트리를 갱신하거나 무효화한다. 대부분의 경우 해당 페이지는 처음 로드되는 것이므로 TLB에는 없고, 차후 메모리 접근 시 페이지 테이블에서 읽혀 TLB에 캐시될 것이다.
	5.	프로세스 재개 – 페이지 폴트로 중단되었던 프로세스의 잘못 실행했던 명령을 **다시 실행(restart)**시킨다 ￼. 이때 앞서 필요한 페이지가 메모리에 로드되었으므로 이번에는 정상적으로 명령이 수행되고 프로그램이 계속 진행된다. CPU의 예외/인터럽트 복구 매커니즘에 의해 프로그램 카운터(PC)가 조정되어 재실행이 투명하게 이루어진다.

以上 과정을 거치면 페이지 폴트는 해결되지만, 그 처리 시간은 **메모리 접근(수십~수백 ns)**에 비해 **디스크 접근(수 백만 ns, 수 ms)**이 매우 느리기 때문에 프로그램 실행 속도가 크게 느려질 수밖에 없다 ￼ ￼. **페이지 부재율(page fault rate)**이 성능에 미치는 영향을 나타내는 공식으로 유효 접근 시간(EAT) 계산식을 들 수 있다 ￼:

EAT = (1 - p) * 메모리 접근 시간 + p * (페이지 부재 처리 시간) ￼

여기서 p는 페이지 부재가 발생하는 확률(0~1 사이 값)이다. 예를 들어 메모리 접근이 200ns, 페이지 폴트 한 번 처리하는데 8ms(8,000,000ns)가 걸린다고 가정하면, p가 0.001(0.1%)만 되어도 EAT는 약 8.2μs로 메모리만 접근할 때보다 40배 이상 느려지고 ￼, p가 0.01(1%)이면 EAT는 80μs 이상으로 400배 가까이 급증한다. 다행히도 일반적인 프로그램에서 페이지 부재율은 매우 낮게 관리되지만(작업 패턴에 따라 다름), 메모리 부족으로 스래싱이 발생하면 p 값이 급상승하여 시스템 성능이 극도로 떨어질 수 있다. 따라서 운영체제는 워킹셋 추적, 페이지 입출력 스케줄링 등의 기법으로 페이지 부재율을 낮추려고 하며, 경우에 따라서는 **과도한 메모리 사용 프로세스를 종료(OOM Killer)**시키는 등의 조치를 취하기도 한다.

Demand Paging의 이점으로는 메모리 공간 절약과 프로그램 로딩 지연 감소, 그리고 다중 프로세스 동시 실행 효율 증가를 들 수 있다 ￼. 실제로 사용되지 않는 페이지는 아예 메모리에 올리지 않으므로 그만큼 메모리 낭비가 줄어들고, 한정된 메모리로 더 많은 프로세스를 수용할 수 있다. 다만, **첫 접근 시 지연(latency)**이라는 대가를 치러야 하므로, 실무에서는 메모리 접근 패턴을 분석하여 예상 필요한 페이지를 미리 읽어오는 **Prepaging(사전 페이징)**을 부분적으로 활용하기도 한다 ￼. Prepaging은 Demand Paging의 반대로 어느 정도 선제적으로 페이지를 적재하여 페이지 폴트를 줄이는 기법인데, 잘못 예측하면 오히려 불필요한 I/O를 늘릴 수 있어 신중한 튜닝을 요구한다 ￼.

Copy-on-Write의 의미와 활용

**Copy-on-Write(COW)**는 운영체제에서 메모리의 효율적 공유를 위해 사용하는 기술로, **“복사는 나중에 한다”**는 아이디어에 기반한다. 구체적으로, 어떤 프로세스들이 동일한 메모리 내용을 읽기 전용으로 공유하고 있다가 그 중 하나가 해당 메모리에 쓰기 작업을 수행하려 할 때 비로소 복사본을 만들어 분리하는 방식이다 ￼. 처음부터 각각 별도의 복사본을 만드는 대신, 쓰기 전까지는 한 개의 물리 메모리를 공유함으로써 불필요한 메모리 낭비와 복사 연산을 줄일 수 있다.

Copy-on-Write가 가장 널리 활용되는 곳은 프로세스 생성이다 ￼. UNIX 계열 운영체제에서 새로운 프로세스를 만들 때 보통 fork() 시스템 호출을 사용하는데, fork()는 호출 프로세스(부모)의 메모리 전체를 복제하여 자식 프로세스를 만든다. 그런데 대개 자식 프로세스는 곧바로 exec()을 호출해 새로운 프로그램을 실행하거나, 부모와 같은 코드를 그대로 읽기만 하고 큰 변경을 하지 않는 경우가 많다. 만약 fork 시 매번 메모리 전체를 깊은 복사한다면 상당한 시간과 메모리 낭비가 발생한다. Copy-on-Write를 사용하면 이 문제를 해결할 수 있다. fork() 호출 시 실제로는 부모의 페이지들을 복사하지 않고, 부모와 자식 프로세스의 페이지 테이블이 모두 동일한 물리 페이지들을 가리키도록 설정한다 ￼. 단, 이때 공유되는 페이지들은 모두 읽기 전용(read-only) 속성으로 표시하여, 이후 부모나 자식이 그 메모리에 쓰기 작업을 시도하면 하드웨어가 다시 페이지 폴트 트랩을 일으키도록 한다 ￼. 페이지 폴트 핸들러는 해당 페이지가 COW 공유 상태임을 확인하고, 그 순간에만 새로운 물리 메모리 페이지를 할당한 뒤 기존 내용을 복사하여 두 프로세스가 각각 별도의 페이지를 갖도록 분리한다 ￼. 그리고 나서 원래 시도했던 쓰기 명령을 재개하면, 이제 분리된 페이지 위에서 안전하게 수행된다. 이렇게 하면 실제로 변경이 발생한 페이지들만 복사하게 되므로, fork()-exec()로 이어지는 경우나 변경이 적은 경우에 엄청난 이득을 얻는다 ￼. 즉, 초기에는 부모-자식이 전 메모리를 공유하므로 fork()가 매우 빠르게 완료되고, 이후에도 바뀌지 않는 많은 페이지는 물리 메모리 한 벌만 유지되므로 메모리 사용량도 절약된다.

Copy-on-Write는 운영체제의 메모리 관리 시스템 전반에 응용되는데, 예를 들어 프로세스 간의 메모리 공유나 가상 머신의 메모리 최적화 등에도 사용된다. Linux 커널은 메모리 페이지의 **참조 카운트(reference count)**를 관리하여, 여러 프로세스가 같은 페이지를 공유할 경우 해당 카운트를 증가시켜 둔다 ￼. Copy-on-Write 상태로 공유되는 페이지는 **쓰기 보호(write-protect)**되어 있으므로 어느 하나가 쓰기를 시도하면 반드시 트랩이 발생하고, 커널은 그때 새로운 페이지를 확보하여 내용을 복사한 뒤 각 프로세스에 분배한다 ￼. 복사가 이루어지면 참조 카운트가 줄어들고, 결과적으로 해당 페이지는 더 이상 공유되지 않게 된다. 이러한 과정은 운영체제 내부에서 투명하게 처리되므로 프로세스 입장에서는 모르는 사이에 일어나지만, 그 효과로 메모리 사용 효율과 프로세스 생성 성능이 대폭 향상된다.

또 다른 활용 예로, Linux에서는 익명 메모리 할당 시에도 Copy-on-Write 기법을 응용한다 ￼. 프로세스가 malloc() 등으로 새로운 메모리를 요청하면 즉시 물리 메모리를 할당하지 않고, 우선 가상 주소 공간에 **공용의 “빈 페이지”**를 매핑해둔다 ￼. 이 페이지는 모두 0으로 채워진 읽기 전용 페이지이며, 실제 여러 프로세스가 한꺼번에 이 빈 페이지를 참조할 수 있다. 그러다가 해당 메모리에 쓰기가 발생하면 그때서야 개별적인 물리 메모리가 할당되고, 마치 초기화된 0이 들어있던 것처럼 동작하게 된다 ￼. 이를 통해 예를 들어 프로세스가 큰 크기의 메모리를 할당만 하고 실제로는 일부만 사용할 경우, 사용되지 않은 부분에 대해 물리 메모리를 아낄 수 있다. 이 방식은 Demand Paging과 유사하게 실제 사용 시점까지 물리 메모리 할당을 지연시키는 전략이며, 리눅스의 과할당(over-commit) 메모리 관리 정책의 한 부분이기도 하다.

정리하면, Copy-on-Write는 메모리의 중복 복사를 피하고 자원을 지연 할당하는 기법으로서, 특히 백엔드 서버 개발 등에서 프로세스 간 데이터 공유나 프로세스 생성 최적화에 관해 자주 언급된다. 예를 들어 서버 프로세스 풀을 미리 가동해 둘 때 fork()를 사용해 빠르게 프로세스를 복제해 두고, 각 자식 프로세스는 Copy-on-Write로 부모의 초기 메모리를 공유하므로 시스템 부하를 낮출 수 있다. 자식이 실제 요청을 처리하면서 필요한 데이터 구조를 변경할 때만 메모리가 복사되어 분리되므로, 초기 상태에서는 수십 개의 프로세스도 부모 프로세스 한 개 분의 메모리만 소비하는 효과를 낸다. 이렇게 OS의 Copy-on-Write 덕분에 프로세스 복제가 비교적 가벼운 연산이 되었고(과거에 비해), 이는 멀티프로세스 기반 서버들이 성능을 유지하는 데 기여하고 있다.

스택, 힙 메모리의 동작과 가상 메모리와의 연관성

프로그램이 런타임에 사용하는 메모리 공간은 **여러 세그먼트(segment)**로 나눠볼 수 있다. 그 중 개발자가 흔히 접하는 것이 **스택 영역(stack)**과 **힙 영역(heap)**이다. 이 둘은 모두 프로세스의 가상 주소 공간 안에 존재하지만, 용도와 동작 방식이 크게 다르다.

스택(stack) 영역은 함수 호출과 지역 변수 저장을 위해 사용되는 메모리 공간이다. 스택은 보통 **고주소(high address)**에서 시작하여 **아래 방향(low address 방향)**으로 성장하며, 함수가 호출될 때마다 **스택 프레임(stack frame)**이 쌓이고 리턴될 때마다 해당 프레임이 제거되는 후입선출(LIFO) 구조로 동작한다 ￼ ￼. 스택에는 함수의 매개변수, 복귀 주소, 그리고 지역 변수들이 저장되고, 이러한 데이터는 함수 실행이 끝나면 자동으로 해제되므로 메모리 관리가 자동적이고 매우 빠르다 ￼. 예를 들어 재귀함수가 100번 중첩 호출되면 그만큼 스택에 100개의 프레임이 순서대로 쌓였다가, 함수들이 반환되면서 역순으로 100개가 차례로 사라진다 ￼. 스택 크기는 운영체제가 정한 한계까지 동적으로 커질 수 있지만 보통 수 MB 정도로 한정되어 있으며, 스택 메모리가 한도를 넘어서 더 성장하려 하면 **스택 오버플로(stack overflow)**가 발생하여 해당 프로세스가 비정상 종료된다. 이는 가상 메모리 차원에서 보면, 스택 끝부분에 미리 설정된 **가드 페이지(보호용 미할당 페이지)**를 건드려서 의도적으로 페이지 폴트를 일으키고, 운영체제가 이를 감지해 “스택 초과”로 간주하기 때문이다. 따라서 너무 깊은 재귀 호출이나 거대한 배열의 재귀 할당 등은 스택 오버플로를 일으킬 수 있으므로 주의해야 한다.

힙(heap) 영역은 동적 메모리 할당을 위해 사용되는 공간이다. 힙은 일반적으로 데이터 세그먼트 끝부분 바로 위의 주소에서 시작하여 **위쪽(높은 주소 방향)**으로 성장한다 ￼. 프로세스가 실행 중 필요에 따라 malloc()/new 등을 호출하면 힙 영역에서 지정한 바이트만큼 할당되고, free()/delete를 호출하면 해당 영역이 해제되어 재사용될 수 있게 된다 ￼ ￼. 힙은 스택과 달리 자동으로 관리되지 않고 프로그래머가 명시적으로 관리해야 하는 영역으로, 할당된 메모리를 해제하지 않으면 프로그램이 종료될 때까지 차지하고 있게 된다. 이러한 실수를 **메모리 누수(memory leak)**라고 하며, 백엔드 서버처럼 오래 실행되는 프로그램에서 누수가 지속되면 결국 가용 메모리가 부족해져 성능 문제가 발생하거나 프로그램이 강제 종료될 수 있다. 힙 영역의 크기는 이론적으로 프로세스 주소 공간에서 스택과 만나기 전까지 계속 늘어날 수 있으며, 현대 OS에서는 필요한 경우 가상 메모리를 이용해 힙을 확장한다. 예컨대 glibc의 malloc() 구현은 내부적으로 brk() 시스템 콜을 사용하여 힙 끝(프로그램 브레이크)을 앞으로 늘려 힙 영역을 확대하거나 ￼ ￼, 큰 할당의 경우 mmap() 시스템 콜로 별도의 익명 메모리를 맵핑하여 힙처럼 활용하기도 한다 ￼. 이때 운영체제는 Demand Paging 기법으로 실제 물리 메모리 할당을 지연시키므로, 힙에 할당된 메모리를 프로그램이 실제로 접근할 때에야 비로소 물리 메모리가 매핑된다.

스택과 힙은 프로세스 주소 공간의 양 끝에서 서로를 향해 성장하는 전통적 구조를 가지며 ￼ ￼, 아래와 같이 메모리 레이아웃을 그려볼 수 있다:

Higher Addresses (높은 주소)

|--------------|
| Stack        |  (▼ 내려감, 함수 호출 시 확장)
|______________|
|              |  (미사용 공간 또는 mmap 영역 등)
|              |
|______________|
| Heap         |  (▲ 올라감, 동적 할당 시 확장)
|   ...        |  <-- Program Break (힙 끝 경계)
|--------------|
| BSS Segment  |  (전역/정적 미초기화 데이터)
|--------------|
| Data Segment |  (전역/정적 초기화 데이터)
|--------------|
| Text Segment |  (프로그램 코드)
|--------------|

Lower Addresses (낮은 주소)

위 그림에서 보듯, **코드 영역(Text)**과 **데이터 영역(Data/BSS)**은 비교적 크기가 고정되어 있지만, 힙과 스택은 동적으로 크기가 변한다 ￼. 운영체제는 프로세스 시작 시 힙과 스택에 초기 크기를 부여하고, 필요하면 가상 메모리를 이용해 더 확장한다. 예를 들어 C 프로그램에서 malloc()으로 많은 메모리를 요청하면 커널에 brk() 호출을 통해 힙을 늘리거나, 큰 경우 mmap()으로 새로운 지역을 할당해준다 ￼. 반대로 free()로 연속된 힙 끝 부분을 모두 해제하면 brk()를 이용해 프로그램 브레이크를 뒤로 줄여 힙 크기를 축소시킬 수도 있다 (그러나 일반적인 메모리 할당자는 메모리 재활용을 내부적으로 관리할 뿐 brk를 줄이는 일은 거의 하지 않는다). 스택도 함수 호출에 따라 한 페이지 한 페이지씩 아래로 내려가다가, 만약 설정된 한계를 넘어서면 운영체제가 자동으로 스택 영역을 한두 페이지 확장해주는 메커니즘도 있다. Linux의 경우 stack segment에 대한 가드 페이지 바로 아래 주소에 접근이 일어나면, 커널이 이를 인지하여 해당 프로세스의 스택을 확장해주는 식이다. 하지만 이러한 확장은 제한적이며(ulimit 등으로 제한됨) 무한정 커지진 않는다.

스택 vs 힙의 차이점을 요약하면 다음과 같다:
	•	관리 주체: 스택은 CPU와 OS가 자동 관리(push/pop 등)하고, 힙은 프로그래머가 수동 관리한다 (필요 시 할당하고 다 쓰면 해제).
	•	할당/해제 속도: 스택은 포인터 이동만으로 빠르게 할당/소멸하지만, 힙은 느린 malloc/free 호출과 내부 관리 오버헤드가 있다.
	•	크기 제한: 스택은 일반적으로 크기가 제한되어 (수 MB 수준) 고정적이고, 힙은 가상 메모리가 허용하는 한 유동적으로 크기를 키울 수 있다 (물론 시스템 전체 메모리 한계는 있다).
	•	구조: 스택은 연속적이며 LIFO 방식, 힙은 빈 공간을 찾아 할당하므로 **단편화(fragmentation)**될 수 있고 임의의 위치에 노드들이 놓인다.
	•	용도: 스택에는 함수의 지역 변수, 매개변수, 복귀 주소 등이 저장되고 함수 끝나면 해제되므로 일시적 데이터에 적합하다. 힙은 동적으로 크기를 조절해야 하는 데이터 구조(예: 연결 리스트, 트리)나 수명이 함수 범위를 넘는 객체들을 저장하는 데 사용한다.

가상 메모리 체계에서, 스택과 힙은 각각 별도의 주소 영역으로 할당되어 서로 충돌하지 않도록 관리된다. 과거에는 스택과 힙이 한 공간을 가운데서 마주보며 성장하여 만나는 경우 충돌이 발생했지만, 현대 OS는 보통 스택과 힙 사이에 여유 공간을 두거나 mmap 등으로 완전히 다른 공간을 활용하므로 그런 충돌은 드물다. 다만 여전히 스택 오버플로나 메모리 누수 같은 문제는 개발자가 신경 써야 하며, 이는 결국 운영체제 관점에서 유효하지 않은 메모리 접근 혹은 과도한 가상 메모리 소비로 나타나기 때문에 디버깅 시 가상 메모리 지식을 응용할 수 있다. 예를 들어 세그멘테이션 폴트의 오류 주소가 매우 높게 찍힌다면 스택 오버플로(스택이 가드 페이지를 넘은 경우)일 수 있고, 프로세스의 가상 메모리 맵(`/proc/<pid>/maps`)을 살펴 동적 할당 영역이 비정상적으로 커져 있으면 힙 메모리 누수를 의심해볼 수 있다. 이런 식으로 백엔드 개발자도 스택/힙 동작과 가상 메모리의 관계를 이해하고 있으면, 성능 최적화나 오류 분석에 큰 도움이 된다.

mmap 등 시스템 콜과 실무 연계 개념

운영체제의 가상 메모리 기능은 프로그래머에게 다양한 시스템 콜 API로 제공되며, 이를 활용하면 실무에서도 효율적인 메모리 조작이 가능하다. 그 중 중요한 하나가 mmap(2) 시스템 콜이다. mmap은 파일이나 디바이스를 프로세스의 가상 메모리 공간에 맵핑하는 기능으로, 메모리 매핑 파일 I/O를 구현한다 ￼. 이 호출을 사용하면 특정 파일 내용을 메모리 주소로 바로 접근할 수 있게 되는데, 실제로는 Demand Paging 기법으로 동작하므로 처음에는 파일 내용을 읽지 않고 있다가 메모리 접근이 일어날 때 비로소 해당 부분을 디스크에서 읽어온다 ￼. 예를 들어 1GB짜리 파일을 mmap으로 맵핑해도 초기에는 별도 물리 메모리 소모가 없고, 프로그램이 해당 메모리(파일 내용)에 접근하는 페이지들만 차차 페이지 캐시에 로드된다. 이로써 프로그래머는 파일을 위한 read()/write() 호출을 일일이 쓰는 대신 메모리를 다루듯 접근할 수 있고, OS가 백그라운드에서 페이지 단위로 I/O를 최적화해 수행하므로 간편성과 성능 두 가지 이득을 얻는다 ￼. 특히 큰 파일의 임의 접근(random access)의 경우, mmap이 필요한 부분만 메모리에 올리는 장점을 통해 오버헤드를 크게 줄일 수 있다.

mmap에는 **익명 맵핑(anonymous mapping)**과 파일 맵핑(file-backed mapping) 두 가지 형태가 있다 ￼. 파일 맵핑은 앞서 설명한 대로 파일 내용을 가상 메모리에 대응시키는 것이고, 익명 맵핑은 특정 파일 없이 빈 메모리 영역을 맵핑하는 방식이다 ￼. 익명 맵핑은 내용이 초기화된 파일이 없으므로 매핑된 메모리의 초기 내용은 0으로 채워지며, malloc() 구현이 큰 메모리 할당 시 종종 mmap(MAP_ANONYMOUS)을 사용하는 것이 이 경우에 해당한다 ￼. 예컨대 glibc의 malloc은 약 수백 KB 이상의 큰 요청이 들어오면 힙 영역 대신 개별 mmap 호출로 커다란 청크를 할당하는데, 이는 메모리 조각화를 줄이고 커널의 가상 메모리 관리 기능을 활용하기 위함이다 ￼. 프로세스가 fork()를 할 때도 익명 맵핑된 메모리는 기본적으로 사본-쓰기 방식으로 동작하며, 여러 프로세스 간에 MAP_SHARED 플래그로 맵핑하면 공유 메모리로 활용할 수도 있다 ￼. 반대로 MAP_PRIVATE로 파일을 맵핑하면 각 프로세스가 변경한 내용이 파일에 반영되지 않고 자체 COW 복사본에만 적용된다 ￼.

mmap의 활용 예시로, 데이터베이스 서버나 캐시 서버에서 대용량 파일(예: 데이터 파일)을 메모리에 맵핑하여 I/O를 효율화하는 경우를 들 수 있다. 이를테면 데이터베이스는 전통적인 read() 호출로 데이터를 읽는 대신 mmap으로 파일을 맵핑해 두고 메모리를 접근하듯 데이터를 참조할 수 있다. 운영체제는 실제로 필요한 페이지들만 메모리에 올리고 안 쓰는 부분은 자동으로 제거(swap out)해주므로, 개발자는 파일이 “자동으로 캐싱”되는 효과를 얻는다 ￼. 많은 현대 **데이터베이스(Storage Engine)**나 검색 엔진, CDN 등이 메모리 맵핑 기법을 활용하고 있으며 ￼, 커널의 페이지 캐시를 그대로 사용하는 장점(중복 캐싱을 피함) 때문에 성능상 유리하다. 다만, mmap을 남용하면 오히려 페이지 폴트 처리가 빈번해지거나, 잘못된 접근으로 프로세스가 크래시(SEGFAULT)할 위험도 있으므로 사용 시 주의가 필요하다. 일반적인 파일 I/O와 달리 mmap된 메모리를 접근하다 예외가 발생하면 (예: 파일 뒷부분을 잘못 접근) SIGSEGV가 발생할 수 있다.

메모리 관련 기타 시스템 콜로 munmap(), mprotect(), madvise() 등이 있다. munmap은 매핑된 메모리를 해제하는 호출로, mmap으로 할당한 메모리를 반환할 때 사용한다 ￼. mprotect는 지정한 메모리 영역의 접근 권한을 변경하는 시스템 콜로, 메모리 페이지를 읽기 전용으로 바꾸거나 실행 불가능하게 만드는 등에 활용할 수 있다 ￼. 예를 들어 JIT 컴파일러가 생성한 코드를 메모리에 쓰고 나서 그 페이지를 실행 가능하게 설정할 때 mprotect를 사용한다. madvise는 프로세스가 자신이 사용할 메모리 패턴을 운영체제에 힌트로 제공하는 호출로, 예컨대 "이 메모리는 순차적으로 접근할 것이니 미리 읽어두어라" 또는 "당분간 사용 안 할 것이니 swap-out 우선 대상이다" 등의 조언을 할 수 있다 ￼. 이러한 고급 기능들은 백엔드 시스템 프로그래밍에서 성능 튜닝이나 특수한 메모리 관리에 쓰인다.

마지막으로, 백엔드 개발에서 가상 메모리 개념이 응용되는 실무 사례를 정리해보면 다음과 같다:
	•	메모리 풀/슬랩 할당자: 가상 메모리를 이용해 큰 메모리 공간을 미리 확보한 다음, 애플리케이션 레벨에서 객체들을 풀링하여 관리하는 기법. 운영체제의 페이지 할당보다 효율적으로 메모리를 관리하기 위해 웹 서버, DB 등은 자체 메모리 풀을 구현하며, 이때 가상 메모리 주소 공간을 적절히 사용한다.
	•	메모리 맵핑 DB: 앞서 언급한대로, mmap을 사용하여 디스크 데이터 파일을 메모리에 직접 맵핑하는 DB (예: MongoDB MMAPv1 엔진 등 과거 사례). 페이지 캐시를 활용하여 별도 버퍼 캐시를 두지 않고 OS에게 메모리 관리 책임을 위임한다.
	•	프로세스 fork/exec 모델 서버: 멀티프로세스로 동작하는 서버(예: 아파치 Pre-fork 모드 등)에서는 fork()로 다수의 자식 프로세스를 만들고, 각 자식은 Copy-on-Write로 부모의 초기 메모리를 공유한다. 이런 구조에서는 많은 프로세스가 생겨도 실제 물리 메모리 증가는 최소화되며, 요청을 처리하면서 필요한 부분만 복사되어 메모리 사용이 늘어나므로 초기 메모리 사용량이 경제적이다.
	•	메모리 제한 및 모니터링: 컨테이너 혹은 C그룹 등을 통해 프로세스의 사용 가능한 메모리를 제한하는 경우, 개발자는 자신의 애플리케이션이 가상 메모리 상에서 할당은 많이 하더라도 실제 접근 패턴이 locality를 가진다면 물리 메모리 사용은 적을 수 있음을 이해하고 과할당을 활용할 수 있다. 하지만 최악의 경우 접근하는 메모리가 너무 많아지면 스래싱이나 OOM 킬러에 의해 종료될 수 있음을 항상 염두에 두어야 한다. 이는 모두 가상 메모리 동작과 연관된 현상이다.

요약하면, **가상 메모리 관련 시스템 콜(mmap, brk, mprotect 등)**과 OS 메커니즘(COW, Paging 등)은 백엔드 개발자가 고성능 시스템을 구현할 때 강력한 도구가 된다. 적절한 활용을 통해 I/O 병목을 줄이거나 메모리 사용 최적화를 달성할 수 있지만, 잘못 사용하면 예상치 못한 페이지 폴트 증가나 메모리 고갈 문제가 생길 수 있으므로 OS 수준 동작까지 이해하고 접근하는 것이 바람직하다.

주요 OS (Linux)에서의 실제 구현 사례

지금까지 가상 메모리의 원리와 요소들을 살펴보았는데, 이를 실제 운영체제(특히 리눅스)에서는 어떻게 구현하고 있는지 간략히 알아보자. Linux는 페이징 기반의 가상 메모리를 사용하며, 현대 CPU의 메모리 관리 기능을 최대한 활용하여 효율을 높이고 있다.
	•	페이지 테이블 구조: Linux는 플랫폼에 따라 다단계 페이지 테이블을 구현한다. x86-64 아키텍처의 경우 4단계 페이지 테이블(또는 활성화된 5-level paging 기능으로 5단계)을 사용하며, cr3 레지스터에 최상위 페이지 디렉터리 포인터를 두고 하위 테이블들을 차례로 탐색하는 하드웨어 메커니즘을 연계한다. 각 프로세스(task)은 mm_struct라는 메모리 관리 구조체를 가지고 있고, 그 안에 페이지 테이블의 시작 주소(페이지 전역 디렉터리, PGD)가 저장되어 있다. 프로세스가 CPU에서 실행될 때 커널은 해당 PGD 주소를 cr3에 로드하여 해당 프로세스의 주소 공간을 활성화한다. 이로써 CPU 메모리 접근은 지정된 페이지 테이블을 참조하게 되고, 다른 프로세스의 페이지는 접근하지 못하게 된다.
	•	가상 주소 영역 분할: 앞서 언급했듯, 32-bit Linux에서는 상위 1GB를 커널 공간, 하위 3GB를 사용자 공간으로 사용한다 ￼. 그래서 사용자 프로세스의 유효 가상 주소는 0x00000000 ~ 0xBFFFFFFF까지이며, 0xC0000000 ~ 0xFFFFFFFF는 커널이 사용한다. 64-bit Linux에서는 커널이 사용하는 영역은 상위쪽의 일부(예: 절대번지 FFFFFFFF80000000h 부근부터)이며, 나머지 아래쪽 넓은 공간을 각 프로세스에 할당한다 ￼. 커널 공간은 모든 프로세스에 공통으로 매핑되는데, 이 영역은 사용자 모드에서 접근이 불가능하고 오직 커널 모드에서만 접근할 수 있도록 보호된다. 이렇게 하는 이유는 커널이 프로세스의 시스템 콜을 실행할 때 빠르게 커널 코드와 데이터를 참조할 수 있게 하기 위함이다. (컨텍스트 전환 없이도 동일한 주소로 접근 가능). 다만 이로 인해 멜트다운 같은 보안 문제가 제기되자, 최근에는 커널 페이지 테이블 격리(KPTI) 기법으로 사용자 공간에서는 아예 커널 공간을 매핑해두지 않고 시스템 콜 진입 시에만 매핑하도록 바뀌었다. 이는 예외적인 상황이고, 논외로 하면 기본 아이디어는 한 주소 공간에 사용자+커널 부분을 나누어 쓰는 것이다.
	•	TLB 및 캐시: Linux는 하드웨어 TLB에 크게 관여하지 않지만, 프로세스 컨텍스트 전환 시 flush_tlb_mm() 등을 호출하여 해당 프로세스의 주소 공간에 대한 TLB 엔트리를 무효화하거나, flush_tlb_page()로 특정 페이지 주소의 TLB를 날리는 일을 한다. 또 다중 코어 환경에서 한 코어가 페이지 테이블을 수정하면 다른 코어들의 TLB에 남아있는 옛 엔트리를 제거하는 TLB Shootdown 메커니즘도 구현되어 있다. 이는 인터럽트나 IPI를 통해 각 CPU에게 TLB flush 명령을 전달하는 방식으로 동작한다.
	•	메모리 할당과 페이징: Linux에서는 프로세스가 malloc()을 호출하면 보통 glibc가 힙 영역을 관리하다가 힙이 모자랄 때 커널의 brk() 시스템 콜을 호출하여 힙 크기를 늘린다. brk()는 프로세스의 프로그램 브레이크를 올려 더 많은 가상 메모리를 할당해주는 역할을 한다. 이때 커널은 해당 가상 주소 구간을 그 프로세스의 페이지 테이블에 유효하지만 아직 물리 프레임 할당되지 않은 상태로 표시해 둔다. 그리고 실제로 프로세스가 그 페이지에 접근하면 그제서야 페이지 폴트가 발생하여 실제 물리 메모리가 할당된다 (Demand Paging의 일종). 또한 glibc malloc은 큰 메모리 요청(mmap threshold를 넘어서는 경우)에 대해 직접 mmap()을 사용해서 별도의 가상 메모리 영역을 할당한다 ￼. 이는 프로세스의 메모리 공간에서 익명 맵 영역으로 나타나며, free() 호출 시 munmap()되어 반환된다.
	•	페이지 교체 정책: Linux 메모리 관리의 핵심은 LRU 리스트이다. 커널은 메모리에 올라와 있는 모든 페이지를 **활성(active)**과 비활성(inactive) 리스트로 관리하며, 새로 적재된 페이지는 inactive 리스트에 넣고 일정 조건을 만족하면 active로 승격시킨다. 페이지가 참조되면 (하드웨어의 Accessed bit, 즉 참조 비트가 세트됨) 커널은 그 정보를 이용해 해당 페이지를 active 리스트로 옮기거나 active 리스트의 뒤로 이동시켜 최근 사용됨을 표시한다. 메모리가 부족하여 페이지를 교체해야 하면, 커널은 inactive 리스트의 앞쪽(오래 참조되지 않은 페이지)부터 대상으로 삼아 swap out 혹은 해제한다. 이 과정에서 Linux는 완전한 LRU 구현 대신 Clock-Pro 알고리즘의 개념을 도입하여, 일정 횟수 이상 참조된 빈도 높은 페이지를 보호하고 빈도 낮은 페이지는 적극적으로 쫓아내도록 튜닝되어 있다 ￼. 결과적으로 자주/최근 쓰인 페이지는 메모리에 남고, 오래 안 쓰인 페이지는 디스크로 내보내져 페이지 부재율을 낮추도록 설계된다.
	•	Demand Paging & Copy-on-Write 구현: Linux는 실행 파일을 로드할 때 프로그램의 각 세그먼트(코드, 데이터)를 실제로 읽지 않고 가상 메모리 영역에 파일을 매핑만 시켜둔다. 그래서 프로세스가 처음 그 코드나 데이터를 접근할 때 페이지 폴트가 발생하며, 그제서야 커널이 해당 바이너리 파일로부터 해당 페이지를 읽어온다. 이를 Lazy Loading이라고 하며, 결국 Linux의 exec은 파일을 곧바로 읽는 것이 아니라 페이지 테이블만 셋업해두는 작업인 셈이다. Copy-on-Write도 마찬가지로 fork 시에 부모의 페이지 테이블 엔트리를 모두 공유 + 읽기전용화하고, mm_struct 등 프로세스의 메모리 디스크립터도 공유 카운트를 증가시켜둔다. Linux의 do_fork() 함수 구현을 보면 페이지는 복사하지 않고 참조만 공유하도록 하고, pte의 상태를 mark as COW로 해둔다. 이후 자식이나 부모가 쓰기 접근 시 page_fault 핸들러에서 해당 PTE의 COW 플래그를 확인하여 위에서 설명한 새 페이지 할당 및 복사 과정을 거친다. 덕분에 Linux는 수많은 프로세스를 빠르게 생성할 수 있고, 특히 vfork() (일시적으로 부모 중단 + exec 기대) 등의 최적화와 결합하면 프로세스 생성-종료가 빈번한 환경에서도 성능을 높인다.
	•	스왑(swap)과 OOM: Linux는 물리 메모리가 부족할 때를 대비해 스왑 영역(swap partition or file)에 일시적으로 페이지를 저장해둘 수 있다. 앞서 페이지 교체에서 언급한 inactive 리스트의 오래된 페이지들은 수정되었을 경우 디스크 swap 영역에 기록되고 메모리에서 제거된다. 이렇게 되면 메모리를 비워 다른 필요한 페이지를 올릴 수 있다. 그러나 프로세스들이 계속 메모리를 요구하면 스왑마저 소진될 수 있는데, 이 상황이 오기 전에 Linux는 OOM (Out-Of-Memory) Killer를 동작시켜 메모리를 많이 쓰는 프로세스를 강제로 종료함으로써 시스템을 살린다. OOM Killer 발동은 곧 해당 프로세스 입장에서 보면 갑자기 SIGKILL을 받고 죽는 것이므로, 백엔드 개발자는 메모리 사용량에 유의하여 OOM이 발생하지 않도록 해야 한다. 특히 컨테이너 환경에서는 주어진 메모리 내에서만 사용해야 하므로, 가상 메모리 할당량과 실제 RSS(resident set size) 사용량을 모니터링하며 서비스를 운영한다.

以上과 같이, Linux의 가상 메모리 시스템은 우리가 논의한 개념들(페이지 테이블, TLB, 페이지 교체, Demand Paging, Copy-on-Write 등)을 실제로 구현하여 사용하고 있다. 이러한 내부 작동을 이해하면, 예컨대 “왜 어떤 경우에 시스템이 디스크 thrashing에 빠지는지”, “프로세스를 fork 할 때 메모리 사용량 변화가 미미한 이유”, “메모리 맵 파일이 페이지 캐시를 통해 공유되는 원리” 등을 명확히 설명할 수 있다. 백엔드 개발은 주로 애플리케이션 레벨 작업이지만, 운영체제 수준의 동작을 이해하는 개발자는 문제 발생 시 근본 원인을 찾거나 성능 최적화 지점을 파악하는 데 훨씬 유리하다.

면접 질문 예시 및 실무 팁

아래는 가상 메모리 주제와 관련하여 면접에서 자주 등장하는 질문들과 그 배경에 대한 간략한 설명이다. 스스로 답을 생각해보고 앞서 정리한 내용을 토대로 대비해두면 도움이 된다.
	1.	Q: 가상 메모리를 사용하는 주된 이유는 무엇인가요?
A: 물리 메모리의 한계를 극복하고 프로세스별 독립된 주소 공간을 제공하기 위해서입니다. 가상 메모리는 디스크를 활용하여 물리 메모리보다 큰 메모리를 사용하는 환상을 제공하고 ￼, 프로세스 간 메모리를 격리하여 안정성과 보안을 높입니다 ￼. 또한 프로그래머가 메모리 관리의 복잡성을 신경 쓰지 않고도 큰 연속 메모리처럼 코딩할 수 있어 개발 편의성도 향상됩니다 ￼.
	2.	Q: TLB(변환 색인 버퍼)는 무엇이며, 왜 필요한가요?
A: TLB는 페이지 테이블 조회를 빠르게 하기 위한 하드웨어 캐시입니다. CPU의 MMU에 내장되어 자주 사용되는 가상->물리 주소 매핑을 저장해두며, 메모리 접근 시 먼저 TLB를 조회하여 히트하면 곧바로 주소 변환이 이루어집니다 ￼. TLB가 없다면 모든 메모리 참조마다 느린 페이지 테이블 메모리 접근이 추가로 필요하므로 성능이 크게 떨어집니다. 즉, TLB는 주소 변환 속도를 향상시켜 가상 메모리의 성능 손실을 줄여주는 필수적인 구성 요소입니다.
	3.	Q: 페이지 폴트(page fault)는 무엇이며, 발생 시 어떤 일이 일어나나요?
A: 페이지 폴트는 프로세스가 접근한 가상 메모리 페이지가 현재 물리 메모리에 없는 경우 발생하는 예외입니다. CPU가 해당 주소를 찾지 못해 운영체제에 도움을 요청하는 신호라고 볼 수 있습니다. 페이지 폴트가 발생하면 운영체제는 일련의 처리과정을 거치는데 ￼, 우선 접근한 주소의 유효성을 검증하고, 문제가 없는 정당한 접근이라면 빈 메모리 프레임을 확보한 뒤 디스크에서 해당 페이지 데이터를 읽어와 메모리에 적재합니다 ￼. 그리고 페이지 테이블을 갱신하고 프로세스를 재개하여, 중단되었던 명령을 다시 실행합니다 ￼. 이 과정에서 디스크 I/O가 수반되므로 페이지 폴트 발생 시 해당 메모리 접근은 수천만 배까지 느려질 수 있고 ￼, 페이지 폴트율이 높아지면 시스템 성능이 급격히 떨어지는 현상(스로싱)이 나타납니다.
	4.	Q: FIFO와 LRU 페이지 교체 알고리즘의 차이를 설명해보세요.
A: FIFO는 메모리에 가장 먼저 들어온 페이지를 먼저 내보내는 알고리즘이고, LRU는 가장 오랫동안 사용되지 않은 페이지를 내보내는 알고리즘입니다 ￼ ￼. FIFO는 구현이 매우 단순하지만, 최근에 자주 쓰이는 페이지라도 오래 있었다는 이유만으로 교체하는 문제가 있어 성능이 좋지 않을 수 있으며 Belady의 Anomaly가 발생하기도 합니다 ￼. LRU는 시간적 지역성을 반영하여 실제 잘 안 쓰이는 페이지를 내보내므로 FIFO보다 일반적으로 페이지 부재율이 낮아 성능이 좋습니다 ￼. 다만 LRU는 사용 순서를 기억해야 해서 구현이 복잡하고 오버헤드가 큽니다 ￼. 운영체제들은 주로 LRU에 가까운 성능을 내면서 FIFO만큼 단순하게 구현할 수 있는 Clock 알고리즘(Second-Chance) 등의 변형을 실무에 사용합니다.
	5.	Q: Copy-on-Write는 어떤 개념이며 운영체제에서 어떻게 활용되나요?
A: Copy-on-Write(COW)는 처음에는 메모리를 공유하고 있다가, 실제 쓰기(write) 시에 복사본을 만들어 분기하는 기법입니다 ￼. 운영체제는 fork() 호출 시 부모 프로세스의 모든 메모리를 자식과 공유하게 하고, 그 페이지들을 읽기 전용으로 표시합니다 ￼. 이후 부모나 자식이 해당 메모리에 쓰려고 하면 페이지 폴트가 발생하고, OS가 그때 새로운 페이지를 할당해 이전 내용을 복사한 뒤 쓰기를 수행하도록 합니다 ￼. 이렇게 하면 자식 프로세스가 메모리를 수정하지 않는 한 물리 메모리 복사가 전혀 일어나지 않으므로 프로세스 생성이 매우 가볍고 빠르게 이루어집니다. 실무에서 COW는 fork 기반 서버에서 메모리 사용 효율을 높이고 프로세스 복제 성능을 향상시키는 핵심 기술로 활용됩니다.
	6.	Q: 스택 영역과 힙 영역은 무엇이고 어떻게 다른가요?
A: 스택은 함수 호출 시 생성되는 지역 변수, 매개변수, 복귀 주소 등을 저장하는 메모리 영역으로, 컴파일러/CPU에 의해 자동으로 할당되고 해제됩니다 ￼. 스택은 보통 높은 주소부터 낮은 방향으로 성장하며, 함수가 리턴하면 해당 스택 프레임이 알아서 사라지므로 관리가 편리합니다. 힙은 동적 할당을 위해 사용되는 메모리 영역으로, 프로그래머가 malloc/new로 할당하고 free/delete로 해제해야 합니다 ￼ ￼. 힙은 일반적으로 낮은 주소부터 높은 방향으로 성장하고, 크기가 필요에 따라 유동적입니다. 스택은 크기 한도가 비교적 작고 LIFO 구조라는 제약이 있지만 접근 속도가 매우 빠르고 함수 실행에 적합하며, 힙은 큰 용량을 쓸 수 있고 복잡한 데이터 구조를 저장할 수 있으나 메모리 단편화와 누수에 유의해야 합니다. 가상 메모리 관점에서 스택과 힙은 서로 반대 방향으로 성장하도록 주소 공간에 배치되어 있습니다 ￼ ￼.
7.	Q: mmap 시스템 호출은 무엇이며 언제 활용하면 좋을까요?
A: mmap은 파일을 프로세스의 가상 메모리에 맵핑하거나, 익명 메모리 영역을 맵핑하는 시스템 콜로, 메모리 매핑 I/O를 가능하게 합니다 ￼. 예를 들어 큰 파일을 mmap으로 맵핑하면 파일 내용을 메모리 주소를 통해 직접 읽고 쓸 수 있는데, 실제 읽기/쓰기 동작은 Demand Paging에 의해 필요한 시점에 커널이 알아서 처리해줍니다 ￼. 이를 활용하면 대용량 파일에 대한 임의 접근 성능이 향상되고, 별도의 read/write 호출 없이 포인터로 다룰 수 있어 코드도 단순해집니다. 따라서 데이터베이스나 멀티미디어 처리 등에서 대용량 파일을 다룰 때, 또는 여러 프로세스 간 메모리 공유가 필요할 때 mmap을 사용하면 좋습니다. 다만, mmap된 메모리를 잘못 접근하면 세그멘테이션 폴트가 발생할 수 있고, I/O 패턴에 따라 오히려 페이지 폴트가 자주 일어나면 성능이 나빠질 수도 있으므로, 순차 접근이 확실한 경우에는 일반 I/O가 나을 수 있습니다. 요약하면, mmap은 “파일을 메모리처럼” 다룰 수 있게 해주는 도구로서, 대용량 파일 캐싱이나 IPC(Shared Memory) 등에 유용합니다.



# 공부하면서 헷갈렸던 부분들

- 페이지 테이블은 어디에 저장될까? PCB?

- TLB은 PCB에 저장되나? 컨텍스트 스위칭이 일어날때 프로세스별 TLB도 같이 복원되나?

- TLB를 선캐싱하기도 할까?
	- [[TLB 선캐싱]]

- 프로세스가 실행되는 시점에 PCB에 저장된 PageTable 시작 주소가 MMU 내부 레지스터에 들어가는건가?
	- TLB는 MMU 내부에 위치한다?

- 논리 주소에 어떤 물리 주소를 맵핑할지 어디서 결정하나?
	- 맵핑에 사용할 물리 주소는 어떤 기준으로 선택할까?
	- [[운영체제에서 물리 메모리를 관리하는 방법]]
	- [[논리 주소에 물리 주소가 할당되는 시점은?]]

- JVM 힙은 OS로부터 이미 할당받은 공간 아닌가?
	- 프로세스의 논리 주소 구간 중 일부를 할당받은 것
	- [[논리주소는 이론상 무한하지만 최대 크기를 제한해서 사용하는 이유]]

- 프로세스 레벨에서 논리주소는 어떻게 할당되지?

- 다단계 페이지테이블이란?
	- 성능이 중요한 경우에는 단일 페이지를 쓰기도 할까?
		- TLB 용량과 계층적 구조 확장
		- Huge Page 사용
			- 메모리 접근 패턴이 연속적이고 쓰기 많은 워크로드에서 자동으로 2MB Huge Page로 병합  
			- ex. Java heap, DBMS의 버퍼 풀 등
		- ASID/PCID를 활용한 TLB flush 최소화

- 커널 영역도 가상 주소를 통해서 관리된다면 프로세스 메모리 영역 일부를 커널 영역으로 사용하는 대신 별도의 메모리 공간을 확보해두는게 효율적이지 않나?

- 프로세스 페이지 테이블 주소와 커널 영역 테이블 주소를 동일한 페이지테이블에서 관리했을때 어떤 문제점이 있을까?
	- [[Meltdown 취약점]]

- TLB hit의 경우 몇 사이클 안에 처리될까? (고속 연관 캐시 구조)

- TLB 교체 알고리즘은 OS에서 실행된다 (O/X)

---

- ? TLB에서도 페이지 테이블처럼 커널 영역이 항상 고정적으로 올라가있을까?

- ? 64비트 컴퓨터에서는 이론상 페이지 폴트로 인한 성능 사이즈를 감안하면 디스크 사이즈만큼 메모리를 사용할 수 있는건가